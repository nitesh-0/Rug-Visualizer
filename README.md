# ğŸ  AI Rug Visualizer - Production Edition

RugUSA-quality rug visualization with **real AI models** - no heuristics, no fallbacks.

## âœ¨ Features

- **MiDaS Depth Estimation** - Intel's state-of-the-art monocular depth
- **SAM Segmentation** - Facebook's Segment Anything Model for floor/furniture
- **Depth-Aware Placement** - Rugs scale realistically with perspective
- **Furniture Occlusion** - Objects properly layer over rugs
- **Real-Time Manipulation** - Drag, scale, rotate with live preview
- **Production Quality** - Modular, type-safe, enterprise-ready code

---

## ğŸš€ Quick Start

### Option 1: Local Setup (Recommended for GPU)

**Requirements:**
- Python 3.9+
- Node.js 18+
- CUDA GPU (optional but recommended)
- 8GB RAM minimum, 16GB recommended

**1. Clone/Setup:**
```bash
# Create project directory
mkdir rug-visualizer && cd rug-visualizer

# Create backend structure
mkdir -p backend/{ai,core,models,uploads,outputs,cache}
mkdir -p frontend/src/{components,services,utils}
```

**2. Backend Setup:**
```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download models (2.4GB SAM model)
python download_models.py

# Start backend
python main.py
```

The backend will:
- Download MiDaS automatically on first run (via torch.hub)
- Preload models (takes ~30 seconds)
- Start API on http://localhost:8000

**3. Frontend Setup:**
```bash
cd ../frontend

# Initialize Vite project (if not exists)
npm create vite@latest . -- --template react

# Install dependencies
npm install lucide-react

# Start frontend
npm run dev
```

Frontend runs on http://localhost:5173

---

### Option 2: Kaggle (GPU Available)

**Why Kaggle?**
- Free GPU (P100)
- Pre-installed PyTorch
- 16GB RAM
- Perfect for AI models

**Setup:**

1. **Create New Notebook:**
   - Go to kaggle.com
   - Click "New Notebook"
   - Enable GPU: Settings â†’ Accelerator â†’ GPU P100

2. **Install Dependencies:**
```python
# Cell 1: Install packages
!pip install -q fastapi uvicorn python-multipart timm
!pip install -q git+https://github.com/facebookresearch/segment-anything.git
```

3. **Upload Backend Files:**
```python
# Cell 2: Create structure
!mkdir -p backend/{ai,core,models,uploads,outputs,cache}
```

Upload all Python files from artifact to respective folders.

4. **Download Models:**
```python
# Cell 3: Download SAM
!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth \
      -O backend/models/sam_vit_h_4b8939.pth
```

5. **Run Backend:**
```python
# Cell 4: Start API
import subprocess
import time

# Start backend in background
proc = subprocess.Popen(['python', 'backend/main.py'])
time.sleep(10)  # Wait for startup

print("Backend running on port 8000")
```

6. **Access API:**
   - Kaggle provides public URLs
   - Use ngrok or Kaggle's URL forwarding
   - Update frontend API_BASE to Kaggle URL

---

## ğŸ“ Project Structure

```
rug-visualizer/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ depth_estimator.py      # MiDaS depth
â”‚   â”‚   â””â”€â”€ sam_segmenter.py        # SAM segmentation
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ compositor.py           # Rug compositing
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ sam_vit_h_4b8939.pth   # Downloaded
â”‚   â”œâ”€â”€ uploads/                    # User uploads
â”‚   â”œâ”€â”€ outputs/                    # Generated images
â”‚   â”œâ”€â”€ cache/                      # Analysis cache
â”‚   â”œâ”€â”€ config.py                   # Configuration
â”‚   â”œâ”€â”€ main.py                     # FastAPI app
â”‚   â”œâ”€â”€ download_models.py          # Model downloader
â”‚   â””â”€â”€ requirements.txt            # Python deps
â”‚
â””â”€â”€ frontend/
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”œâ”€â”€ RoomUpload.jsx      # Upload component
    â”‚   â”‚   â”œâ”€â”€ CanvasDisplay.jsx   # Canvas renderer
    â”‚   â”‚   â”œâ”€â”€ RugCatalog.jsx      # Rug selector
    â”‚   â”‚   â””â”€â”€ RugControls.jsx     # Adjustment controls
    â”‚   â”œâ”€â”€ services/
    â”‚   â”‚   â””â”€â”€ api.js              # API client
    â”‚   â”œâ”€â”€ utils/
    â”‚   â”‚   â””â”€â”€ rugGenerator.js     # Pattern generator
    â”‚   â”œâ”€â”€ App.jsx                 # Main app
    â”‚   â””â”€â”€ main.jsx                # Entry point
    â”œâ”€â”€ package.json
    â””â”€â”€ vite.config.js
```

---

## ğŸ”§ Configuration

**Backend** (`backend/config.py`):
```python
MIDAS_MODEL = "DPT_Large"     # Best quality
SAM_MODEL = "vit_h"           # ViT-H (2.4GB)
MAX_IMAGE_SIZE = 2048         # Resize large images
DEVICE = "cuda"               # or "cpu"
```

**Frontend** (`.env`):
```env
VITE_API_URL=http://localhost:8000
```

---

## ğŸ¯ Usage

1. **Upload Room Photo**
   - Click upload area
   - Select clear, well-lit room photo
   - Wait for AI analysis (~10-20 seconds)

2. **Select Rug**
   - Choose from catalog
   - Rug appears on floor automatically

3. **Adjust Placement**
   - Drag rug to reposition
   - Scale: Slider to resize
   - Rotate: Slider to rotate
   - Depth Scaling: Toggle for perspective
   - Occlusion: Toggle for furniture layering

4. **Export**
   - Click "Export HD"
   - Saves final composited image

---

## ğŸ› Troubleshooting

### Backend Issues

**"Failed to load MiDaS model"**
```bash
pip install timm
# MiDaS requires timm for transformers
```

**"SAM checkpoint not found"**
```bash
python download_models.py
# Or manually download from GitHub
```

**"CUDA out of memory"**
```python
# In config.py:
DEVICE = "cpu"
MAX_IMAGE_SIZE = 1024  # Reduce size
```

**Backend not starting:**
```bash
# Check port 8000 is free
lsof -ti:8000 | xargs kill -9  # Mac/Linux
netstat -ano | findstr :8000   # Windows

# Check logs
python main.py
# Look for error messages
```

### Frontend Issues

**"Network Error"**
- Ensure backend is running (http://localhost:8000)
- Check CORS settings in backend
- Verify API_BASE in api.js

**"Rug not appearing"**
- Check browser console for errors
- Ensure analysis completed (green badge)
- Try refreshing page

---

## ğŸ¨ Adding Custom Rugs

Edit `frontend/src/utils/rugGenerator.js`:

```javascript
export const RUG_PATTERNS = [
  // ... existing patterns
  {
    id: 7,
    name: 'Your Pattern',
    pattern: 'custom',
    colors: ['#HEX1', '#HEX2', '#HEX3']
  }
];

// Add pattern generator:
case 'custom':
  // Your drawing code
  ctx.fillStyle = colors[0];
  // ...
  break;
```

---

## ğŸ—ï¸ Architecture

### AI Pipeline

```
Room Image
    â†“
1. MiDaS Depth Estimation
    â”œâ”€â†’ Depth Map (HÃ—W float32)
    â†“
2. SAM Floor Segmentation
    â”œâ”€â†’ Floor Mask (HÃ—W bool)
    â”œâ”€â†’ Floor Corners (4-8 points)
    â†“
3. SAM Furniture Segmentation
    â”œâ”€â†’ Furniture Masks (List[HÃ—W bool])
    â†“
4. Cache Results
    â””â”€â†’ .npz file
```

### Compositing Pipeline

```
Rug + Parameters
    â†“
1. Depth-Based Scaling
    â”œâ”€â†’ Sample depth at position
    â”œâ”€â†’ Calculate scale factor
    â†“
2. Transform Rug
    â”œâ”€â†’ Scale
    â”œâ”€â†’ Rotate
    â”œâ”€â†’ Perspective Warp
    â†“
3. Generate Shadow
    â”œâ”€â†’ From rug alpha
    â”œâ”€â†’ Blur based on depth
    â†“
4. Composite Layers
    â”œâ”€â†’ Room (base)
    â”œâ”€â†’ Shadow (on floor)
    â”œâ”€â†’ Rug (on floor)
    â”œâ”€â†’ Furniture (occlusion)
    â†“
Final Image
```

---

## ğŸ“Š Performance

**First Run** (cold start):
- Model loading: ~30s
- MiDaS download: ~500MB (automatic)
- SAM checkpoint: ~2.4GB (manual)

**Subsequent Runs:**
- Upload + Analysis: 10-20s
- Compositing: 0.3-1s (real-time)
- Export: <0.1s

**GPU vs CPU:**
- GPU (CUDA): 10x faster analysis
- CPU: Acceptable for single use
- Kaggle GPU: Free & fast

---

## ğŸš¢ Deployment

### Docker (Production)

```dockerfile
FROM python:3.10-slim

WORKDIR /app
COPY backend/ .
RUN pip install -r requirements.txt
RUN python download_models.py

EXPOSE 8000
CMD ["python", "main.py"]
```

### Cloud Options

- **AWS EC2**: g4dn.xlarge (GPU)
- **Google Cloud**: n1-standard-4 + T4 GPU
- **Kaggle**: Free notebooks with P100
- **Vercel**: Frontend only (separate backend)

---

## ğŸ“ API Documentation

### POST `/upload/room`
Upload room image
- **Body**: `multipart/form-data` with `file`
- **Returns**: `{image_id, width, height}`

### POST `/analyze/complete`
Analyze room with AI
- **Body**: `form-data` with `image_id`
- **Returns**: `{floor_mask_base64, depth_map_base64, floor_confidence, furniture_count}`
- **Time**: 10-20s

### POST `/composite/realtime`
Composite rug in real-time
- **Body**: JSON with `{room_image_id, rug_data_url, position_x, position_y, base_scale, rotation, use_depth, use_furniture_occlusion}`
- **Returns**: `{image_base64}`
- **Time**: 0.3-1s

---

## ğŸ¤ Contributing

This is a production-ready template. To improve:

1. **Better Models**: Try Depth-Anything-v2 instead of MiDaS
2. **More Rugs**: Add real product images
3. **Advanced Features**: Lighting adjustment, texture mapping
4. **Mobile**: Touch controls, responsive design

---

## ğŸ“œ License

MIT - Use freely for commercial projects

---

## ğŸ†˜ Support

**Issues?**
1. Check troubleshooting section
2. Enable debug logging: `logging.basicConfig(level=logging.DEBUG)`
3. Check model files exist in `backend/models/`
4. Verify GPU available: `torch.cuda.is_available()`

**Contact:** Open GitHub issue or discussion

---

Made with â¤ï¸ for production AI applications